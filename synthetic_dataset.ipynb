{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "synthetic_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "2KdRJGOCYUAU",
        "_Fvr7kPLXmuV",
        "58T6NOToXlac",
        "MkWi33BYp8Il",
        "fDiIkHxbsYw4",
        "BOJFUHRGLCvc",
        "RU5xTlXtIgL1",
        "XxV8ji5VcdEl",
        "5_b9mvt7AJnW",
        "4Kq2FLyVPy0d",
        "0iotBjUCDByV",
        "VFbr3aVMWs8S",
        "EbflMjqWUk_4",
        "BOh16vIMxsTQ",
        "UlnsJa-d0V-p",
        "Hecqe7ZJb78D"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katyamineeva/person-detection-from-aerial-drone/blob/master/synthetic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4jQhSXAXHkg",
        "colab_type": "text"
      },
      "source": [
        "#Synthetic datasets creation\n",
        "\n",
        "Note that this notebook is adapted to Google.Colab usege."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KdRJGOCYUAU",
        "colab_type": "text"
      },
      "source": [
        "## Loading required libs, modules, models and datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js2NWaw-VlU_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "12150932-b930-46b5-98b0-c0c2f213aa38"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import skimage.io\n",
        "from skimage import img_as_float, img_as_uint, img_as_int\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "import itertools\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.lines as lines\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.patches import Polygon\n",
        "import pickle\n",
        "import shutil\n",
        "from IPython.display import clear_output\n",
        "import csv\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fvr7kPLXmuV",
        "colab_type": "text"
      },
      "source": [
        "###Local modules loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-MSx-6ej-XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('/content/drive/My Drive/project')\n",
        "from utils_main_project.local_paths import *\n",
        "from utils_main_project.saving_loading import *\n",
        "from utils_main_project.visualization import *\n",
        "from utils_main_project.global_constants import *\n",
        "\n",
        "sys.path.append(paths['mask-rcnn-tf'])\n",
        "\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "from mrcnn.utils import extract_bboxes\n",
        "from samples.coco import coco"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58T6NOToXlac",
        "colab_type": "text"
      },
      "source": [
        "### Functions for trained Mask R-CNN  and MS COCO dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTF6T6MRcwzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_trained_mrcnn():        \n",
        "    class InferenceConfig(coco.CocoConfig):\n",
        "        GPU_COUNT = 1\n",
        "        IMAGES_PER_GPU = 1\n",
        "        \n",
        "    logs_dir = os.path.join(paths['mask-rcnn-tf'], 'logs')\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=logs_dir, config=InferenceConfig())\n",
        "    \n",
        "    weights_path = os.path.join(paths['mask-rcnn-tf'], 'mask_rcnn_coco.h5')\n",
        "    model.load_weights(weights_path, by_name=True)\n",
        "    return model\n",
        "\n",
        "def load_coco():\n",
        "    dataset = coco.CocoDataset()\n",
        "    dataset.load_coco(paths['coco'], \"val\")\n",
        "    dataset.prepare()\n",
        "    return dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkWi33BYp8Il",
        "colab_type": "text"
      },
      "source": [
        "### Content of paths dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hQB9W8398k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_paths_dict(paths, 'paths')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiIkHxbsYw4",
        "colab_type": "text"
      },
      "source": [
        "## Algorithms for cropping people"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOJFUHRGLCvc",
        "colab_type": "text"
      },
      "source": [
        "###  Ð¡ropped persons filtering and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2iZsbUCumA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_person_id_coco(class_id):\n",
        "    return class_id == PERSON_CLASS_ID_COCO\n",
        "\n",
        "def person_detected(class_ids_detected):\n",
        "    for class_id in class_ids_detected:\n",
        "        if is_person_id_coco(class_id):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def person_is_too_small(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return ((h <= MIN_PERSON_HEIGHT) or (w <= MIN_PERSON_WIDTH))\n",
        "\n",
        "def person_is_not_full_body(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return not (MIN_PERSON_RATIO <= h / w <= MAX_PERSON_RATIO)\n",
        "\n",
        "def model_rejects(model, image):\n",
        "    r = model.detect([image], verbose=0)[0]\n",
        "    for class_id, score in zip(r['class_ids'], r['scores']):\n",
        "        if is_person_id_coco(class_id) and score > 0.95:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def person_is_too_big(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return ((h > MAX_PERSON_HEIGHT) or (w > MAX_PERSON_WIDTH))\n",
        "\n",
        "def downsample_image_n_mask(image, mask):\n",
        "    shape = mask.shape\n",
        "    \n",
        "    ratio_h = max(shape[0], shape[1]) / PERFECT_PERSON_HEIGHT\n",
        "    ratio_w = min(shape[0], shape[1]) / PERFECT_PERSON_WIDTH\n",
        "    ratio = max(ratio_h, ratio_w)\n",
        "    new_shape = (int(shape[0] / ratio), int(shape[1] / ratio))\n",
        "\n",
        "    image = skimage.transform.resize(image, (*new_shape, 3))\n",
        "    mask = np.round(skimage.transform.resize(mask, new_shape))\n",
        "    \n",
        "    return image, mask\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU5xTlXtIgL1",
        "colab_type": "text"
      },
      "source": [
        "### Functions for cropping people from an images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mHvLQt6Hgp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_person(image, mask, model_filter=False, model=None):\n",
        "    not_empty_i = (mask.sum(axis=0) > 0)\n",
        "    not_empty_j = (mask.sum(axis=1) > 0)\n",
        "    cropped_shape = (sum(not_empty_j), sum(not_empty_i))\n",
        "\n",
        "    if person_is_too_small(cropped_shape) or person_is_not_full_body(cropped_shape):\n",
        "        return\n",
        "\n",
        "    squared_mask = np.full(mask.shape, True)\n",
        "    squared_mask = ((squared_mask * not_empty_i).T * not_empty_j).T                \n",
        "\n",
        "    masked_person = image * mask.reshape(*mask.shape, -1)\n",
        "    cropped_person = masked_person[squared_mask].reshape(*cropped_shape, -1)\n",
        "    cropped_mask = mask[squared_mask].reshape(cropped_shape)\n",
        "    \n",
        "    if model_filter and model_rejects(model, cropped_person):\n",
        "        return\n",
        "\n",
        "    if person_is_too_big(cropped_shape):\n",
        "        cropped_person, cropped_mask = downsample_image_n_mask(cropped_person, cropped_mask)        \n",
        "\n",
        "    if skimage.exposure.is_low_contrast(cropped_person):\n",
        "        return\n",
        "        \n",
        "    return cropped_person, cropped_mask\n",
        "\n",
        "\n",
        "def crop_people_from_image(image, cnt_generated, class_ids, mask, output_path, model_filter=False, model=None):\n",
        "    cropped_images_path = output_path + '/images'\n",
        "    if (not os.path.exists(cropped_images_path)):\n",
        "        os.mkdir(cropped_images_path)\n",
        "        \n",
        "    cropped_masks_path = output_path + '/masks'\n",
        "    if (not os.path.exists(cropped_masks_path)):\n",
        "        os.mkdir(cropped_masks_path)\n",
        "\n",
        "    for ind, class_id in enumerate(class_ids):\n",
        "        if is_person_id_coco(class_id):\n",
        "            person_mask = mask[: , : , ind]     \n",
        "            crop_result = crop_person(image, person_mask, model_filter, model)\n",
        "            \n",
        "            if crop_result != None:\n",
        "                cropped_person, cropped_mask = crop_result\n",
        "                cnt_generated += 1\n",
        "                save_object_pickle(cropped_mask, cropped_masks_path, mask_id2filename(cnt_generated))\n",
        "                save_image(cropped_person, cropped_images_path, image_id2filename(cnt_generated))\n",
        "    return cnt_generated\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxV8ji5VcdEl",
        "colab_type": "text"
      },
      "source": [
        "### Function for cropping people using Mask R-CNN predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhufaYisCcu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_crops_people_from_dataset(dataset_path, output_path, number_to_generate=2400, display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)\n",
        "\n",
        "    model = load_trained_mrcnn()\n",
        "\n",
        "    cnt_generated = 0\n",
        "    for path in get_files_paths_list(dataset_path):\n",
        "        image = skimage.io.imread(path)\n",
        "        r = model.detect([image], verbose=0)[0]\n",
        "        cnt_generated = crop_people_from_image(image, cnt_generated, r['class_ids'], r['masks'], output_path, model_filter=True, model=model)\n",
        "\n",
        "        if display_progress and (cnt_generated % 10) == 0:\n",
        "            clear_output()\n",
        "            print(\"generated\", cnt_generated)\n",
        "        if cnt_generated > number_to_generate:\n",
        "            return \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_b9mvt7AJnW",
        "colab_type": "text"
      },
      "source": [
        "### Function for cropping people from COCO dataset using masks from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkuWYZjV45VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_people_from_coco(output_path, number_to_generate=3000, model_filter=False, display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)    \n",
        "    \n",
        "    dataset = load_coco()\n",
        "    if model_filter:\n",
        "        model = load_trained_mrcnn()\n",
        "    else:\n",
        "        model = None\n",
        "    \n",
        "    cnt_generated = 0    \n",
        "    for image_id in dataset.image_ids:\n",
        "        try:\n",
        "            image = dataset.load_image(image_id)\n",
        "            mask, class_ids = dataset.load_mask(image_id)\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "        cnt_generated = crop_people_from_image(image, cnt_generated, class_ids, mask, output_path, model_filter, model)\n",
        "\n",
        "        if (display_progress):\n",
        "            clear_output()\n",
        "            print(\"generated\", cnt_generated, \"images\")\n",
        "        if cnt_generated > number_to_generate:\n",
        "            return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kq2FLyVPy0d",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm for synthetic data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iotBjUCDByV",
        "colab_type": "text"
      },
      "source": [
        "### Generating non-intersecting locations for people on a background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sepb6xh8WS_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def segments_intersect(l1, l2):\n",
        "    l1, l2 = min(l1, l2), max(l1, l2)\n",
        "    return (l1[0] <= l2[0] <= l1[1])\n",
        "\n",
        "def rectangles_intersect(r1, r2):    \n",
        "    h1 = (r1[1], r1[3])\n",
        "    h2 = (r2[1], r2[3])\n",
        "    \n",
        "    v1 = (r1[0], r1[2])\n",
        "    v2 = (r2[0], r2[2])\n",
        "    \n",
        "    return int(segments_intersect(h1, h2) and segments_intersect(v1, v2))\n",
        "\n",
        "def there_are_intersections_in_prefix(rectangles, i):\n",
        "    if len(rectangles) < 2:\n",
        "        return False\n",
        "    for j in range(i):\n",
        "        if rectangles_intersect(rectangles[i], rectangles[j]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "def locate_cropped_people(background_shape, images_shapes):\n",
        "    images_num = len(images_shapes)\n",
        "    \n",
        "    heights = images_shapes.T[0]\n",
        "    widths = images_shapes.T[1]\n",
        "    \n",
        "    b_height = background_shape[0] - max(heights)\n",
        "    b_width = background_shape[1] - max(widths)\n",
        "\n",
        "    rectangles = np.zeros((images_num, 4), dtype=np.int)\n",
        "\n",
        "    def generate_rectangle(i):\n",
        "        rectangles[i][0] = randint(b_height)\n",
        "        rectangles[i][1] = randint(b_width)\n",
        "        rectangles[i][2] = rectangles[i][0] + heights[i]\n",
        "        rectangles[i][3] = rectangles[i][1] + widths[i]\n",
        "    \n",
        "    def generate_locations(i):\n",
        "        if i == images_num:\n",
        "            return 0\n",
        "        \n",
        "        generate_rectangle(i)\n",
        "        cnt_tries = 0\n",
        "        while (there_are_intersections_in_prefix(rectangles, i)):\n",
        "            generate_rectangle(i)\n",
        "            cnt_tries += 1\n",
        "            if cnt_tries > 10:\n",
        "                generate_locations(i - 1)\n",
        "        generate_locations(i + 1)\n",
        "    \n",
        "    generate_locations(0)\n",
        "    return rectangles\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPnRi76ZDRYY",
        "colab_type": "text"
      },
      "source": [
        "### Synthetic data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ses3Z0KBTHWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_person_to_background(background, image, mask, location):\n",
        "    mask = np.array(mask, dtype=np.uint8)\n",
        "\n",
        "    b_shape = background.shape\n",
        "    b_height, b_width, _ = b_shape\n",
        "    m_shape = (b_height, b_width, 1)\n",
        "    s_shape = (b_height, b_width)\n",
        "    \n",
        "    top, left = location\n",
        "    img_height, img_width = mask.shape\n",
        "    bottom, right = top + img_height, left + img_width\n",
        "    \n",
        "    squared_mask = np.full(s_shape, True)\n",
        "    h_mask = (left <= np.arange(b_width)) * (np.arange(b_width) < right)\n",
        "    v_mask = (top <= np.arange(b_height)) * (np.arange(b_height) < bottom)\n",
        "    squared_mask *= h_mask\n",
        "    squared_mask = (squared_mask.T * v_mask).T\n",
        "    \n",
        "    mask_extended = np.full(s_shape, 0, dtype=np.uint8)\n",
        "    \n",
        "    mask_extended[squared_mask] = mask.reshape(-1)\n",
        "    mask_extended = mask_extended.reshape(m_shape)\n",
        "\n",
        "    image_extended = np.full(b_shape, 0, dtype=np.uint8)\n",
        "    image_extended[squared_mask] = image.reshape(-1, 3)\n",
        "    image_extended = image_extended.reshape(b_shape)\n",
        "\n",
        "    res = image_extended + (1 - mask_extended) * background\n",
        "    return res, mask_extended\n",
        "    \n",
        "    \n",
        "def create_sythetic_image(background_path, cropped_people_path, output_path, image_id):\n",
        "    background = skimage.io.imread(background_path)\n",
        "        \n",
        "    created_images_path = output_path + '/images'\n",
        "    if not os.path.exists(created_images_path):\n",
        "        os.mkdir(created_images_path)\n",
        "        \n",
        "    created_masks_path = output_path + '/masks'\n",
        "    if not os.path.exists(created_masks_path):\n",
        "        os.mkdir(created_masks_path)\n",
        "    \n",
        "    people_num = randint(1, 5)\n",
        "    people_ids = list(np.random.choice(image_ids_list(cropped_people_path), people_num))\n",
        "    \n",
        "    people_images, masks = get_images_n_masks(cropped_people_path,\n",
        "                                              return_val='objects',\n",
        "                                              return_iterator=False,\n",
        "                                              ids=people_ids)\n",
        "    \n",
        "    people_shapes = np.array([[image.shape[0], image.shape[1]] for image in people_images])\n",
        "        \n",
        "    # top left coordinates\n",
        "    people_bboxes = locate_cropped_people((background.shape[0], background.shape[1]), people_shapes)\n",
        "    people_locations = np.stack((people_bboxes.T[0], people_bboxes.T[1])).T\n",
        "    \n",
        "    full_masks = []\n",
        "    for image, mask, location in zip(people_images, masks, people_locations):\n",
        "        background, full_mask = add_person_to_background(background, image, mask, location)\n",
        "        full_masks.append(full_mask)\n",
        "        \n",
        "    save_image(background, created_images_path, image_id2filename(image_id))\n",
        "    save_object_pickle(full_masks, created_masks_path, mask_id2filename(image_id))\n",
        "    save_bboxes(people_bboxes, output_path, image_id)\n",
        "    \n",
        "    \n",
        "\n",
        "def create_sythetic_dataset(cropped_people_path, output_path, number_to_generate='unlimited', display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)\n",
        "    cnt_generated = 0\n",
        "    for backgrounds_path in get_subdirs_paths_list(paths['backgrounds']):       \n",
        "        for background_path in get_files_paths_list(backgrounds_path):\n",
        "            create_sythetic_image(background_path, cropped_people_path, output_path, cnt_generated)\n",
        "            cnt_generated += 1\n",
        "            if (display_progress and (cnt_generated % 50 == 0)):\n",
        "                clear_output()\n",
        "                print(\"generated\", cnt_generated, \"images\")\n",
        "                \n",
        "            if (number_to_generate != 'unlimited') and (cnt_generated > number_to_generate):\n",
        "                return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDdUutpbtb1s",
        "colab_type": "text"
      },
      "source": [
        "## Launch creation of synthetic images based on VisDrone2018 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBghTC4OCexu",
        "colab_type": "text"
      },
      "source": [
        "Cropping people form VisDrone2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wenq8BF1S1c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_path = paths['visdrone'] + '/images'\n",
        "output_path = paths['visdrone_cropped_people']\n",
        "\n",
        "model_crops_people_from_dataset(source_path, output_path, display_progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DOOPsHKB8uD",
        "colab_type": "text"
      },
      "source": [
        "Generation of synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE-ZmsS9cdRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cropped_people_path = paths['visdrone_cropped_people']\n",
        "output_path = paths['visdrone_synthetic_images']\n",
        "\n",
        "create_sythetic_dataset(cropped_people_path, output_path, number_to_generate=800, display_progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFkr6KZnuaLM",
        "colab_type": "text"
      },
      "source": [
        "## Launch creation of synthetic data images based on MS COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvUOiA7FXrul",
        "colab_type": "text"
      },
      "source": [
        "Cropping people from COCO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHJfda5WqkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_path = paths['coco_cropped_people']\n",
        "crop_people_from_coco(output_path, model_filter=True, number_to_generate=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFbr3aVMWs8S",
        "colab_type": "text"
      },
      "source": [
        "Synthetic images generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWKnY1t4xAOI",
        "colab_type": "code",
        "outputId": "0d76730c-31a9-43d1-96d7-549828a91d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cropped_people_path = paths['coco_cropped_people']\n",
        "output_path = paths['coco_synthetic_images']\n",
        "\n",
        "create_sythetic_dataset(cropped_people_path, output_path, number_to_generate=800, display_progress=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generated 800 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbflMjqWUk_4",
        "colab_type": "text"
      },
      "source": [
        "# Model testing on synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLkrKiurgyNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_gt_bbox(id, bbox):\n",
        "    [y, x, y2, x2] = bbox\n",
        "    gt_bb = BoundingBox(imageName=str(id),\n",
        "                        classId=str(PERSON_CLASS_ID_COCO),\n",
        "                        x=x, y=y, w=x2, h=y2,\n",
        "                        typeCoordinates=CoordinatesType.Absolute,\n",
        "                        bbType=BBType.GroundTruth,\n",
        "                        format=BBFormat.XYX2Y2)\n",
        "    return gt_bb\n",
        "\n",
        "def init_dt_bbox(id, bbox, score):\n",
        "    [y, x, y2, x2] = bbox\n",
        "    dt_bb = BoundingBox(imageName=str(id),\n",
        "                        classId=str(PERSON_CLASS_ID_COCO),\n",
        "                        x=x, y=y, w=x2, h=y2,\n",
        "                        typeCoordinates=CoordinatesType.Absolute,\n",
        "                        bbType=BBType.Detected,\n",
        "                        classConfidence=score,\n",
        "                        format=BBFormat.XYX2Y2)\n",
        "    return dt_bb\n",
        "\n",
        "\n",
        "def get_bboxes(model, dataset_path, display_progress=False, ids=None):\n",
        "    bb = BoundingBoxes()\n",
        "    gt_bboxes = load_bboxes(dataset_path)\n",
        "    \n",
        "    if ids == None:\n",
        "        ids = image_ids_list(dataset_path)\n",
        "    \n",
        "    \n",
        "    for id, image_path in get_ids_n_images(dataset_path, return_val='paths', ids=ids):\n",
        "        image = skimage.io.imread(image_path)\n",
        "\n",
        "        # ground truth\n",
        "        for bbox in gt_bboxes[id]:\n",
        "            bb.addBoundingBox(init_gt_bbox(id, bbox))\n",
        "            \n",
        "        # detections\n",
        "        r = model.detect([image])[0]\n",
        "        \n",
        "        dt_bboxes = extract_bboxes(r['masks'])\n",
        "        for class_id, bbox, score in zip(r['class_ids'], dt_bboxes, r['scores']):\n",
        "            if is_person_id_coco(class_id):\n",
        "                bb.addBoundingBox(init_dt_bbox(id, bbox, score))\n",
        "                \n",
        "        if display_progress and id % 50 == 0:\n",
        "            clear_output()\n",
        "            print(id, \"images processed\")\n",
        "    return bb\n",
        "\n",
        "\n",
        "def evaluate_model(bb, iou_tr=0.3):\n",
        "    eval = Evaluator()\n",
        "    metrics = eval.GetPascalVOCMetrics(bb, IOUThreshold=iou_tr)[0]\n",
        "    \n",
        "    print(\"AP: \", metrics['AP'])\n",
        "    print('total positives:', metrics['total positives'])\n",
        "    print('total TP:', metrics['total TP'])\n",
        "    print('total FP:', metrics['total FP'])\n",
        "    \n",
        "    eval.PlotPrecisionRecallCurve(bb, IOUThreshold=iou_tr,\n",
        "                                  method=MethodAveragePrecision.EveryPointInterpolation, \n",
        "                                  showAP=True, showInterpolatedPrecision=True) \n",
        "    return metrics       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOh16vIMxsTQ",
        "colab_type": "text"
      },
      "source": [
        "## Testing Mask R-CNN on VisDrone synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTLpG-Zw1Mob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bb1 = get_bboxes(load_trained_mrcnn(), paths['visdrone_synthetic_images'], display_progress=True)\n",
        "mc1 = evaluate_model(bb1)\n",
        "sAP = mc1['AP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-GlcYztxTfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mc1 = evaluate_model(bb1, iou_tr=0.5)\n",
        "sAP += mc1['AP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfqzWfzAxg64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mc1 = evaluate_model(bb1, iou_tr=0.75)\n",
        "sAP += mc1['AP']\n",
        "print('mAP:', sAP / 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlnsJa-d0V-p",
        "colab_type": "text"
      },
      "source": [
        "## Testing Mask R-CNN on COCO synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMoBLdnCsp71",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bb2 = get_bboxes(load_trained_mrcnn(), paths['coco_synthetic_images'], display_progress=True)\n",
        "mc2 = evaluate_model(bb2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFFGL3mThNbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mc2 = evaluate_model(bb2, iou_tr=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLKU5EEGleau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mc2 = evaluate_model(bb2, iou_tr=0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hecqe7ZJb78D",
        "colab_type": "text"
      },
      "source": [
        "# Testing locations generation\n",
        "\n",
        "This part is just to make sure that generated locations indeed don't intersect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5E--Q-h7SiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def person_random_shape():\n",
        "    w = randint(7, 12)\n",
        "    h = randint(17, 25)\n",
        "    if randint(2) == 0:\n",
        "        return [w, h]\n",
        "    return [h, w]\n",
        "\n",
        "# n -- number of rectangels to be placed\n",
        "# n is generated randomly within n_limits semi-interval\n",
        "def test_locations_generation(n_limits=(5, 7), iter_num=1):\n",
        "    n = randint(*n_limits)\n",
        "\n",
        "    persons_shapes = np.array([person_random_shape() for i in range(n)])\n",
        "    \n",
        "    bh_limits = (np.array([30, 35]) * (1 + n / 10))\n",
        "    bw_limits = (np.array([45, 55]) * (1 + n / 10))\n",
        "    \n",
        "    background_shape = (randint(*bh_limits), randint(*bw_limits))\n",
        "    \n",
        "    rectangles = locate_cropped_people(background_shape, persons_shapes)\n",
        "    \n",
        "    draw_rectangles(background_shape, rectangles)\n",
        "    if n <= len(NAMED_COLORS):\n",
        "        for i in range(n):\n",
        "            rectangle = rectangles[i]\n",
        "            (top, left, bottom, right) = rectangles[i]\n",
        "            str1 = NAMED_COLORS[i] + ' rectangle:'\n",
        "            str2 = \"(top, left) = \" + str((top, left))\n",
        "            str3 = \"(bottom, right)= \" + str((bottom, right))\n",
        "            print(\"{:20} {:<25} {:<25}\".format(str1, str2, str3))\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a583f79e-e362-4fe3-9a05-6a1eb5409808",
        "id": "6y4Z8zS-pY0S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "test_locations_generation()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADfCAYAAADmzyjKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWOiS1Ij/AyfC62z1S/ncAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "green rectangle:     (top, left) = (1, 32)     (bottom, right)= (22, 43)\n",
            "red rectangle:       (top, left) = (13, 44)    (bottom, right)= (37, 53)\n",
            "orange rectangle:    (top, left) = (13, 55)    (bottom, right)= (21, 73)\n",
            "blue rectangle:      (top, left) = (12, 7)     (bottom, right)= (35, 18)\n",
            "yellow rectangle:    (top, left) = (0, 9)      (bottom, right)= (7, 31) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}