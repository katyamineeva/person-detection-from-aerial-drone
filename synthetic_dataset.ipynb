{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "synthetic_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "MkWi33BYp8Il",
        "fDiIkHxbsYw4",
        "BOJFUHRGLCvc",
        "RU5xTlXtIgL1",
        "XxV8ji5VcdEl",
        "5_b9mvt7AJnW",
        "4Kq2FLyVPy0d",
        "0iotBjUCDByV",
        "NDdUutpbtb1s",
        "rFkr6KZnuaLM",
        "VFbr3aVMWs8S",
        "Hecqe7ZJb78D"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katyamineeva/person-detection-from-aerial-drone/blob/master/synthetic_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4jQhSXAXHkg",
        "colab_type": "text"
      },
      "source": [
        "#Synthetic datasets creation\n",
        "\n",
        "Note that this notebook is adapted to Google.Colab usege."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KdRJGOCYUAU",
        "colab_type": "text"
      },
      "source": [
        "## Loading required libs, modules, models and datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js2NWaw-VlU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import skimage.io\n",
        "from skimage import img_as_float, img_as_uint, img_as_int\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "import itertools\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.lines as lines\n",
        "import matplotlib.image as mpimg\n",
        "from matplotlib.patches import Polygon\n",
        "import pickle\n",
        "import shutil\n",
        "from IPython.display import clear_output\n",
        "import csv\n",
        "import cv2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fvr7kPLXmuV",
        "colab_type": "text"
      },
      "source": [
        "###Local modules loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-MSx-6ej-XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('/content/drive/My Drive/project')\n",
        "from utils_main_project.local_paths import *\n",
        "from utils_main_project.saving_loading import *\n",
        "from utils_main_project.visualization import *\n",
        "from utils_main_project.global_constants import *\n",
        "\n",
        "sys.path.append(paths['mask-rcnn-tf'])\n",
        "\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "from mrcnn.utils import extract_bboxes\n",
        "from samples.coco import coco"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58T6NOToXlac",
        "colab_type": "text"
      },
      "source": [
        "### Functions for trained Mask R-CNN  and MS COCO dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTF6T6MRcwzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_trained_mrcnn():        \n",
        "    class InferenceConfig(coco.CocoConfig):\n",
        "        GPU_COUNT = 1\n",
        "        IMAGES_PER_GPU = 1\n",
        "        \n",
        "    logs_dir = os.path.join(paths['mask-rcnn-tf'], 'logs')\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=logs_dir, config=InferenceConfig())\n",
        "    \n",
        "    weights_path = os.path.join(paths['mask-rcnn-tf'], 'mask_rcnn_coco.h5')\n",
        "    model.load_weights(weights_path, by_name=True)\n",
        "    return model\n",
        "\n",
        "def load_coco():\n",
        "    dataset = coco.CocoDataset()\n",
        "    dataset.load_coco(paths['coco'], \"val\")\n",
        "    dataset.prepare()\n",
        "    return dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkWi33BYp8Il",
        "colab_type": "text"
      },
      "source": [
        "### Content of paths dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hQB9W8398k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_paths_dict(paths, 'paths')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiIkHxbsYw4",
        "colab_type": "text"
      },
      "source": [
        "## Algorithms for cropping people"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOJFUHRGLCvc",
        "colab_type": "text"
      },
      "source": [
        "###  Ð¡ropped persons filtering and processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2iZsbUCumA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_person_id_coco(class_id):\n",
        "    return class_id == PERSON_CLASS_ID_COCO\n",
        "\n",
        "def person_detected(class_ids_detected):\n",
        "    for class_id in class_ids_detected:\n",
        "        if is_person_id_coco(class_id):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def person_is_too_small(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return ((h <= MIN_PERSON_HEIGHT) or (w <= MIN_PERSON_WIDTH))\n",
        "\n",
        "def person_is_not_full_body(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return not (MIN_PERSON_RATIO <= h / w <= MAX_PERSON_RATIO)\n",
        "\n",
        "def model_rejects(model, image):\n",
        "    r = model.detect([image], verbose=0)[0]\n",
        "    for class_id, score in zip(r['class_ids'], r['scores']):\n",
        "        if is_person_id_coco(class_id) and score > 0.95:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def person_is_too_big(shape):\n",
        "    h = max(shape[0], shape[1])\n",
        "    w = min(shape[0], shape[1])\n",
        "    return ((h > MAX_PERSON_HEIGHT) or (w > MAX_PERSON_WIDTH))\n",
        "\n",
        "def downsample_image_n_mask(image, mask):\n",
        "    shape = mask.shape\n",
        "    \n",
        "    ratio_h = max(shape[0], shape[1]) / PERFECT_PERSON_HEIGHT\n",
        "    ratio_w = min(shape[0], shape[1]) / PERFECT_PERSON_WIDTH\n",
        "    ratio = max(ratio_h, ratio_w)\n",
        "    new_shape = (int(shape[0] / ratio), int(shape[1] / ratio))\n",
        "\n",
        "    image = skimage.transform.resize(image, (*new_shape, 3))\n",
        "    mask = np.round(skimage.transform.resize(mask, new_shape))\n",
        "    \n",
        "    return image, mask\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU5xTlXtIgL1",
        "colab_type": "text"
      },
      "source": [
        "### Functions for cropping people from an images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mHvLQt6Hgp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_person(image, mask, model_filter=False, model=None):\n",
        "    not_empty_i = (mask.sum(axis=0) > 0)\n",
        "    not_empty_j = (mask.sum(axis=1) > 0)\n",
        "    cropped_shape = (sum(not_empty_j), sum(not_empty_i))\n",
        "\n",
        "    if person_is_too_small(cropped_shape) or person_is_not_full_body(cropped_shape):\n",
        "        return\n",
        "\n",
        "    squared_mask = np.full(mask.shape, True)\n",
        "    squared_mask = ((squared_mask * not_empty_i).T * not_empty_j).T                \n",
        "\n",
        "    masked_person = image * mask.reshape(*mask.shape, -1)\n",
        "    cropped_person = masked_person[squared_mask].reshape(*cropped_shape, -1)\n",
        "    cropped_mask = mask[squared_mask].reshape(cropped_shape)\n",
        "    \n",
        "    if model_filter and model_rejects(model, cropped_person):\n",
        "        return\n",
        "\n",
        "    if person_is_too_big(cropped_shape):\n",
        "        cropped_person, cropped_mask = downsample_image_n_mask(cropped_person, cropped_mask)        \n",
        "\n",
        "    if skimage.exposure.is_low_contrast(cropped_person):\n",
        "        return\n",
        "        \n",
        "    return cropped_person, cropped_mask\n",
        "\n",
        "\n",
        "def crop_people_from_image(image, cnt_generated, class_ids, mask, output_path, model_filter=False, model=None):\n",
        "    cropped_images_path = output_path + '/images'\n",
        "    if (not os.path.exists(cropped_images_path)):\n",
        "        os.mkdir(cropped_images_path)\n",
        "        \n",
        "    cropped_masks_path = output_path + '/masks'\n",
        "    if (not os.path.exists(cropped_masks_path)):\n",
        "        os.mkdir(cropped_masks_path)\n",
        "\n",
        "    for ind, class_id in enumerate(class_ids):\n",
        "        if is_person_id_coco(class_id):\n",
        "            person_mask = mask[: , : , ind]     \n",
        "            crop_result = crop_person(image, person_mask, model_filter, model)\n",
        "            \n",
        "            if crop_result != None:\n",
        "                cropped_person, cropped_mask = crop_result\n",
        "                cnt_generated += 1\n",
        "                save_object_pickle(cropped_mask, cropped_masks_path, mask_id2filename(cnt_generated))\n",
        "                save_image(cropped_person, cropped_images_path, image_id2filename(cnt_generated))\n",
        "    return cnt_generated\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxV8ji5VcdEl",
        "colab_type": "text"
      },
      "source": [
        "### Function for cropping people using Mask R-CNN predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhufaYisCcu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_crops_people_from_dataset(dataset_path, output_path, number_to_generate=2400, display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)\n",
        "\n",
        "    model = load_trained_mrcnn()\n",
        "\n",
        "    cnt_generated = 0\n",
        "    for path in get_files_paths_list(dataset_path):\n",
        "        image = skimage.io.imread(path)\n",
        "        r = model.detect([image], verbose=0)[0]\n",
        "        cnt_generated = crop_people_from_image(image, cnt_generated, r['class_ids'], r['masks'], output_path, model_filter=True, model=model)\n",
        "\n",
        "        if display_progress and (cnt_generated % 10) == 0:\n",
        "            clear_output()\n",
        "            print(\"generated\", cnt_generated)\n",
        "        if cnt_generated > number_to_generate:\n",
        "            return \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_b9mvt7AJnW",
        "colab_type": "text"
      },
      "source": [
        "### Function for cropping people from COCO dataset using masks from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkuWYZjV45VZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_people_from_coco(output_path, number_to_generate=3000, model_filter=False, display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)    \n",
        "    \n",
        "    dataset = load_coco()\n",
        "    if model_filter:\n",
        "        model = load_trained_mrcnn()\n",
        "    else:\n",
        "        model = None\n",
        "    \n",
        "    cnt_generated = 0    \n",
        "    for image_id in dataset.image_ids:\n",
        "        try:\n",
        "            image = dataset.load_image(image_id)\n",
        "            mask, class_ids = dataset.load_mask(image_id)\n",
        "        except:\n",
        "            continue\n",
        "            \n",
        "        cnt_generated = crop_people_from_image(image, cnt_generated, class_ids, mask, output_path, model_filter, model)\n",
        "\n",
        "        if (display_progress):\n",
        "            clear_output()\n",
        "            print(\"generated\", cnt_generated, \"images\")\n",
        "        if cnt_generated > number_to_generate:\n",
        "            return "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kq2FLyVPy0d",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm for synthetic data generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iotBjUCDByV",
        "colab_type": "text"
      },
      "source": [
        "### Generating non-intersecting locations for people on a background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sepb6xh8WS_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def segments_intersect(l1, l2):\n",
        "    l1, l2 = min(l1, l2), max(l1, l2)\n",
        "    return (l1[0] <= l2[0] <= l1[1])\n",
        "\n",
        "def rectangles_intersect(r1, r2):    \n",
        "    h1 = (r1[1], r1[3])\n",
        "    h2 = (r2[1], r2[3])\n",
        "    \n",
        "    v1 = (r1[0], r1[2])\n",
        "    v2 = (r2[0], r2[2])\n",
        "    \n",
        "    return int(segments_intersect(h1, h2) and segments_intersect(v1, v2))\n",
        "\n",
        "def there_are_intersections_in_prefix(rectangles, i):\n",
        "    if len(rectangles) < 2:\n",
        "        return False\n",
        "    for j in range(i):\n",
        "        if rectangles_intersect(rectangles[i], rectangles[j]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "    \n",
        "def locate_cropped_people(background_shape, images_shapes):\n",
        "    images_num = len(images_shapes)\n",
        "    \n",
        "    heights = images_shapes.T[0]\n",
        "    widths = images_shapes.T[1]\n",
        "    \n",
        "    b_height = background_shape[0] - max(heights)\n",
        "    b_width = background_shape[1] - max(widths)\n",
        "\n",
        "    rectangles = np.zeros((images_num, 4), dtype=np.int)\n",
        "\n",
        "    def generate_rectangle(i):\n",
        "        rectangles[i][0] = randint(b_height)\n",
        "        rectangles[i][1] = randint(b_width)\n",
        "        rectangles[i][2] = rectangles[i][0] + heights[i]\n",
        "        rectangles[i][3] = rectangles[i][1] + widths[i]\n",
        "    \n",
        "    def generate_locations(i):\n",
        "        if i == images_num:\n",
        "            return 0\n",
        "        \n",
        "        generate_rectangle(i)\n",
        "        cnt_tries = 0\n",
        "        while (there_are_intersections_in_prefix(rectangles, i)):\n",
        "            generate_rectangle(i)\n",
        "            cnt_tries += 1\n",
        "            if cnt_tries > 10:\n",
        "                generate_locations(i - 1)\n",
        "        generate_locations(i + 1)\n",
        "    \n",
        "    generate_locations(0)\n",
        "    return rectangles\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPnRi76ZDRYY",
        "colab_type": "text"
      },
      "source": [
        "### Synthetic data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ses3Z0KBTHWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_person_to_background(background, image, mask, location):\n",
        "    mask = np.array(mask, dtype=np.uint8)\n",
        "\n",
        "    b_shape = background.shape\n",
        "    b_height, b_width, _ = b_shape\n",
        "    m_shape = (b_height, b_width, 1)\n",
        "    s_shape = (b_height, b_width)\n",
        "    \n",
        "    top, left = location\n",
        "    img_height, img_width = mask.shape\n",
        "    bottom, right = top + img_height, left + img_width\n",
        "    \n",
        "    squared_mask = np.full(s_shape, True)\n",
        "    h_mask = (left <= np.arange(b_width)) * (np.arange(b_width) < right)\n",
        "    v_mask = (top <= np.arange(b_height)) * (np.arange(b_height) < bottom)\n",
        "    squared_mask *= h_mask\n",
        "    squared_mask = (squared_mask.T * v_mask).T\n",
        "    \n",
        "    mask_extended = np.full(s_shape, 0, dtype=np.uint8)\n",
        "    \n",
        "    mask_extended[squared_mask] = mask.reshape(-1)\n",
        "    mask_extended = mask_extended.reshape(m_shape)\n",
        "\n",
        "    image_extended = np.full(b_shape, 0, dtype=np.uint8)\n",
        "    image_extended[squared_mask] = image.reshape(-1, 3)\n",
        "    image_extended = image_extended.reshape(b_shape)\n",
        "\n",
        "    res = image_extended + (1 - mask_extended) * background\n",
        "    return res, mask_extended\n",
        "    \n",
        "    \n",
        "def create_sythetic_image(background_path, cropped_people_path, output_path, image_id):\n",
        "    background = skimage.io.imread(background_path)\n",
        "        \n",
        "    created_images_path = output_path + '/images'\n",
        "    if not os.path.exists(created_images_path):\n",
        "        os.mkdir(created_images_path)\n",
        "        \n",
        "    created_masks_path = output_path + '/masks'\n",
        "    if not os.path.exists(created_masks_path):\n",
        "        os.mkdir(created_masks_path)\n",
        "    \n",
        "    people_num = randint(1, 5)\n",
        "    people_ids = list(np.random.choice(image_ids_list(cropped_people_path), people_num))\n",
        "    \n",
        "    people_images, masks = get_images_n_masks(cropped_people_path,\n",
        "                                              return_val='objects',\n",
        "                                              return_iterator=False,\n",
        "                                              ids=people_ids)\n",
        "    \n",
        "    people_shapes = np.array([[image.shape[0], image.shape[1]] for image in people_images])\n",
        "        \n",
        "    # top left coordinates\n",
        "    people_bboxes = locate_cropped_people((background.shape[0], background.shape[1]), people_shapes)\n",
        "    people_locations = np.stack((people_bboxes.T[0], people_bboxes.T[1])).T\n",
        "    \n",
        "    full_masks = []\n",
        "    for image, mask, location in zip(people_images, masks, people_locations):\n",
        "        background, full_mask = add_person_to_background(background, image, mask, location)\n",
        "        full_masks.append(full_mask)\n",
        "        \n",
        "    save_image(background, created_images_path, image_id2filename(image_id))\n",
        "    save_object_pickle(full_masks, created_masks_path, mask_id2filename(image_id))\n",
        "    save_bboxes(people_bboxes, output_path, image_id)\n",
        "    \n",
        "    \n",
        "\n",
        "def create_sythetic_dataset(cropped_people_path, output_path, number_to_generate='unlimited', display_progress=False):\n",
        "    if (not os.path.exists(output_path)):\n",
        "        os.mkdir(output_path)\n",
        "    cnt_generated = 0\n",
        "    for backgrounds_path in get_subdirs_paths_list(paths['backgrounds']):       \n",
        "        for background_path in get_files_paths_list(backgrounds_path):\n",
        "            create_sythetic_image(background_path, cropped_people_path, output_path, cnt_generated)\n",
        "            cnt_generated += 1\n",
        "            if (display_progress and (cnt_generated % 50 == 0)):\n",
        "                clear_output()\n",
        "                print(\"generated\", cnt_generated, \"images\")\n",
        "                \n",
        "            if (number_to_generate != 'unlimited') and (cnt_generated > number_to_generate):\n",
        "                return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDdUutpbtb1s",
        "colab_type": "text"
      },
      "source": [
        "## Launch creation of synthetic images based on VisDrone2018 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBghTC4OCexu",
        "colab_type": "text"
      },
      "source": [
        "Cropping people form VisDrone2018"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wenq8BF1S1c0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source_path = paths['visdrone'] + '/images'\n",
        "output_path = paths['visdrone_cropped_people']\n",
        "\n",
        "model_crops_people_from_dataset(source_path, output_path, display_progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DOOPsHKB8uD",
        "colab_type": "text"
      },
      "source": [
        "Generation of synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE-ZmsS9cdRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cropped_people_path = paths['visdrone_cropped_people']\n",
        "output_path = paths['visdrone_synthetic_images']\n",
        "\n",
        "create_sythetic_dataset(cropped_people_path, output_path, number_to_generate=800, display_progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFkr6KZnuaLM",
        "colab_type": "text"
      },
      "source": [
        "## Launch creation of synthetic data images based on MS COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvUOiA7FXrul",
        "colab_type": "text"
      },
      "source": [
        "Cropping people from COCO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHJfda5WqkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_path = paths['coco_cropped_people']\n",
        "crop_people_from_coco(output_path, model_filter=True, number_to_generate=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFbr3aVMWs8S",
        "colab_type": "text"
      },
      "source": [
        "Synthetic images generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWKnY1t4xAOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cropped_people_path = paths['coco_cropped_people']\n",
        "output_path = paths['coco_synthetic_images']\n",
        "\n",
        "create_sythetic_dataset(cropped_people_path, output_path, number_to_generate=800, display_progress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hecqe7ZJb78D",
        "colab_type": "text"
      },
      "source": [
        "# Testing locations generation\n",
        "\n",
        "This part is just to make sure that generated locations indeed don't intersect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5E--Q-h7SiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def person_random_shape():\n",
        "    w = randint(7, 12)\n",
        "    h = randint(17, 25)\n",
        "    if randint(2) == 0:\n",
        "        return [w, h]\n",
        "    return [h, w]\n",
        "\n",
        "# n -- number of rectangels to be placed\n",
        "# n is generated randomly within n_limits semi-interval\n",
        "def test_locations_generation(n_limits=(5, 7), iter_num=1):\n",
        "    n = randint(*n_limits)\n",
        "\n",
        "    persons_shapes = np.array([person_random_shape() for i in range(n)])\n",
        "    \n",
        "    bh_limits = (np.array([30, 35]) * (1 + n / 10))\n",
        "    bw_limits = (np.array([45, 55]) * (1 + n / 10))\n",
        "    \n",
        "    background_shape = (randint(*bh_limits), randint(*bw_limits))\n",
        "    \n",
        "    rectangles = locate_cropped_people(background_shape, persons_shapes)\n",
        "    \n",
        "    draw_rectangles(background_shape, rectangles)\n",
        "    if n <= len(NAMED_COLORS):\n",
        "        for i in range(n):\n",
        "            rectangle = rectangles[i]\n",
        "            (top, left, bottom, right) = rectangles[i]\n",
        "            str1 = NAMED_COLORS[i] + ' rectangle:'\n",
        "            str2 = \"(top, left) = \" + str((top, left))\n",
        "            str3 = \"(bottom, right)= \" + str((bottom, right))\n",
        "            print(\"{:20} {:<25} {:<25}\".format(str1, str2, str3))\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6y4Z8zS-pY0S",
        "colab": {}
      },
      "source": [
        "test_locations_generation()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}